{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q streamlit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ow1PXc4cg5fL",
        "outputId": "03f803ad-feb9-438c-f7fc-6e1141700cde"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.0/83.0 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "cidj1yYlgBUh"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "import streamlit as st"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download NLTK data (if not already downloaded)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZ6gvMm6gGcv",
        "outputId": "2d4a777f-35f1-4348-c5e1-944668724d25"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the admission data from the CSV file\n",
        "data = pd.read_csv('/content/admission_data - Sheet1.csv')"
      ],
      "metadata": {
        "id": "V-VaksTTgORT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract queries and responses from the DataFrame\n",
        "admission_queries = data['Query'].tolist()\n",
        "admission_responses = data['Intent'].tolist()\n"
      ],
      "metadata": {
        "id": "wSmd9mRogZJX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize and preprocess the admission queries\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()"
      ],
      "metadata": {
        "id": "a81O5DvQgdWS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    tokens = [stemmer.stem(token) for token in tokens]\n",
        "    tokens = [token.lower() for token in tokens if token.lower() not in stop_words]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "preprocessed_queries = [preprocess_text(query) for query in admission_queries]"
      ],
      "metadata": {
        "id": "DVgska-1ghFT"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create a TF-IDF vectorizer and transform the queries\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(preprocessed_queries)\n"
      ],
      "metadata": {
        "id": "4DTP7_EQgkgF"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to handle user queries\n",
        "def handle_admission_query(user_query):\n",
        "    preprocessed_user_query = preprocess_text(user_query)\n",
        "    tfidf_user_query = vectorizer.transform([preprocessed_user_query])\n",
        "\n",
        "    # Calculate cosine similarities between user query and admission queries\n",
        "    similarities = cosine_similarity(tfidf_user_query, tfidf_matrix)\n",
        "\n",
        "    # Find the most similar admission query\n",
        "    most_similar_index = similarities.argmax()\n",
        "    if similarities[0][most_similar_index] > 0.2:\n",
        "        return admission_responses[most_similar_index]\n",
        "    else:\n",
        "        return \"I'm sorry, I don't have information about that specific query.\""
      ],
      "metadata": {
        "id": "_F1B_eI4gnwN"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Streamlit UI\n",
        "st.title(\"Admission Chatbot\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lsr7yE_XgsZY",
        "outputId": "e3416dc6-7880-4bde-ac95-644d9a5807b2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-04-05 09:08:58.262 \n",
            "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
            "  command:\n",
            "\n",
            "    streamlit run /usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py [ARGUMENTS]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeltaGenerator()"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_query = st.text_input(\"Enter your question:\")\n",
        "if st.button(\"Ask\"):\n",
        "    response = handle_admission_query(user_query)\n",
        "    st.text(\"Chatbot: \" + response)\n",
        ""
      ],
      "metadata": {
        "id": "UCgHHRBnhIcW"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1ftCrAU5hQdM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}